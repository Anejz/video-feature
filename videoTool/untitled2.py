# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yQ7hW9TohvZLw-T_Thb_ceNcVCTA1FE5
"""

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# pip install pandas
# pip install git+https://github.com/openai/whisper.git
# pip install pytube
# pip install numpy
# pip install pinecone-client

# @title
import os
import torch
import whisper
import pinecone
import numpy as np
import pandas as pd
from pytube import YouTube

def video_to_audio(video_url, destination):
    # Get the video
    video = YouTube(video_url)
    # Convert video to Audio
    audio = video.streams.filter(only_audio=True).first()
    # Save to destination
    output = audio.download(output_path = destination)
    name, ext = os.path.splitext(output)
    new_file = name + '.mp3'
    # Replace spaces with "_"
    new_file = new_file.replace(" ", "_")
    # Change the name of the file
    os.rename(output, new_file)
    return new_file

#!mkdir audio_data

# Create URL column
audio_path = "audio_data"

# truncated list of links
list_videos = ["https://www.youtube.com/watch?v=CXiAoaMPXdU"]
# Create dataframe
transcription_df = pd.DataFrame(list_videos, columns=['URLs'])
transcription_df.head()

transcription_df["file_name"] = transcription_df["URLs"].apply(
    lambda url: video_to_audio(url, audio_path)
)
transcription_df.head()

#!nvidia-smi

# Set the device
device = "cuda" if torch.cuda.is_available() else "cpu"

# Load the model
whisper_model = whisper.load_model("large", device=device)

def audio_to_text(audio_file):
   return whisper_model.transcribe(audio_file)["text"]

# Apply the function to all the audio files
transcription_df["transcriptions"] = transcription_df["file_name"].apply(lambda f_name: audio_to_text(f_name))

# Show the first five rows
transcription_df.head()

import textwrap
wrapper = textwrap.TextWrapper(width=60)
first_transcription = transcription_df.iloc[0]["transcriptions"]
formatted_transcription = wrapper.fill(text=first_transcription)
# Check first transcription
print(formatted_transcription)

print(transcription_df.shape) 
# (7, 3)

# Install the library 
#!pip install openai

# import openai
import openai
# Set up the OpenAI key
openai.api_key = "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"
def get_embeddings(text_to_embed):
 response = openai.embeddings.create(
     model= "text-embedding-3-large",
     input=[text_to_embed]
 ).data[0].embedding
 # Extract the AI output embedding as a list of floats
 embedding = response
 return embedding

transcription_df["embedding"] = transcription_df["transcriptions"].astype(str).apply(get_embeddings)

print(transcription_df.head())

#!pip install pinecone-client

pc = Pinecone(api_key="b9a2a0b8-1752-4d35-94e5-91b2788886ab")
index = pc.Index("video-transcriptions")

transcription_df["vector_id"] = transcription_df.index
transcription_df["vector_id"] = transcription_df["vector_id"].apply(str)

# Get all the metadata
final_metadata = []
for i in range(len(transcription_df)):
    final_metadata.append({
    'ID':  i,
    'url': transcription_df.iloc[i].URLs,
    'transcription': transcription_df.iloc[i].transcriptions
})
audio_IDs = transcription_df.vector_id.tolist()
audio_embeddings = [arr for arr in transcription_df.embedding]
# Create the single list of dictionary format to insert
data_to_upsert = list(zip(audio_IDs, audio_embeddings, final_metadata))
# Upload the final data
index.upsert(vectors = data_to_upsert)
# Show information about the vector index
index.describe_index_stats()